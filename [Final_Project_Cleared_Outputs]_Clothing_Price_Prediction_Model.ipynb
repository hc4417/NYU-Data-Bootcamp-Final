{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWIacweRrf_h"
      },
      "source": [
        "# Clothing Price Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMNhY9rpk469"
      },
      "source": [
        "**Import La Garconne Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17VD6kPGikK3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/hc4417/NYU-Data-Bootcamp-Final/refs/heads/main/lagarconne_products_cleaned.csv'\n",
        "column_names = ['Retailer', 'Vendor', 'Category', 'Name', 'Price',\n",
        "                'Product Link', 'Description', 'Product Details',\n",
        "                'Description Lexical Diversity', 'Description Length',\n",
        "                'Materials', 'Size Guide', 'Luxury Tier', 'Number of pictures',\n",
        "                'Image Links']\n",
        "\n",
        "lagar_df = pd.read_csv(url, names = column_names, skiprows=1)\n",
        "lagar_df.head()"
      ],
      "metadata": {
        "id": "FeTkuujmQXYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajk4v2HaroWX"
      },
      "source": [
        "###Building the Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix7ygIVhT-Pq"
      },
      "outputs": [],
      "source": [
        "# Setup & imports\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.inspection import PartialDependenceDisplay, permutation_importance\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow3j6qJlliKz"
      },
      "source": [
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87UKQ89NPQFJ"
      },
      "outputs": [],
      "source": [
        "# Image Embedding\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "model.eval()\n",
        "\n",
        "image_embeddings = []\n",
        "\n",
        "for url in lagar_df['Image Links']:\n",
        "    if url:\n",
        "      # Load image from URL\n",
        "      img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "      # Preprocess and move to device\n",
        "      inputs = processor(images=img, return_tensors=\"pt\")\n",
        "\n",
        "      with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token\n",
        "        image_embeddings.append(emb[0])\n",
        "\n",
        "    else:\n",
        "        # Use zeros if no image\n",
        "        image_embeddings.append(np.zeros(model.config.hidden_size))\n",
        "\n",
        "image_embeddings = np.array(image_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsgh1NEWrvJY"
      },
      "outputs": [],
      "source": [
        "X = lagar_df[['Category', 'Description', 'Number of pictures', 'Materials', 'Size Guide']]\n",
        "y = lagar_df['Price']\n",
        "\n",
        "# Train, test, split data (causes some data loss)\n",
        "X_train, X_test, y_train, y_test, img_train, img_test = train_test_split(\n",
        "    X, y, image_embeddings, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert image train and test to vertical array\n",
        "img_train = np.vstack(img_train)\n",
        "img_test  = np.vstack(img_test)\n",
        "\n",
        "# Clean text columns\n",
        "X_train['Description'] = X_train['Description'].fillna(\"missing\").astype(str)\n",
        "X_test['Description']  = X_test['Description'].fillna(\"missing\").astype(str)\n",
        "\n",
        "# Ensure non-empty strings\n",
        "X_train['Materials'] = X_train['Materials'].apply(\n",
        "    lambda lst: ' '.join([str(x).replace(\"'\", \"\").replace('\"', '').replace(' ', '') for x in lst])\n",
        "                if isinstance(lst, list) and len(lst) > 0 else 'missing'\n",
        ")\n",
        "X_test['Materials'] = X_test['Materials'].apply(\n",
        "    lambda lst: ' '.join([str(x).replace(\"'\", \"\").replace('\"', '').replace(' ', '') for x in lst])\n",
        "                if isinstance(lst, list) and len(lst) > 0 else 'missing'\n",
        ")\n",
        "\n",
        "# Preprocess categorical\n",
        "ohe = OneHotEncoder(drop = 'if_binary', sparse_output = False, handle_unknown='ignore')\n",
        "\n",
        "# Preprocess num\n",
        "num = StandardScaler()\n",
        "\n",
        "# Preprocess long text (TfidfVectorizer turns words into numbers)\n",
        "desc = Pipeline(steps = [\n",
        "    ('function_transform', FunctionTransformer(lambda x: x['Description'], validate = False)),\n",
        "    ('Tfidf', TfidfVectorizer(max_features = 300, stop_words=None ))\n",
        "])\n",
        "\n",
        "mat = Pipeline(steps = [\n",
        "    ('function_transform', FunctionTransformer(lambda x: x['Materials'], validate = False)), # Turn materials list into string\n",
        "    ('Tfidf', TfidfVectorizer(max_features = 200, stop_words=None))\n",
        "])\n",
        "\n",
        "# Encode\n",
        "encoder = make_column_transformer(\n",
        "    (ohe, ['Size Guide', 'Category']),\n",
        "    (num, ['Number of pictures']),\n",
        "    (desc, ['Description']),\n",
        "    (mat, ['Materials']),\n",
        "     remainder = 'passthrough')\n",
        "\n",
        "X_train_encoded_base = encoder.fit_transform(X_train)\n",
        "X_test_encoded_base = encoder.transform(X_test)\n",
        "\n",
        "X_train_encoded_base = X_train_encoded_base.toarray()\n",
        "X_test_encoded_base = X_test_encoded_base.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMY1-bfFiVM6"
      },
      "outputs": [],
      "source": [
        "# Combine X_trains\n",
        "X_train_encoded = np.hstack([X_train_encoded_base, img_train])\n",
        "X_test_encoded = np.hstack([X_test_encoded_base, img_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjbDmIrsWTMM"
      },
      "source": [
        "**Baselines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eDdAo4ar288"
      },
      "outputs": [],
      "source": [
        "# Baseline 1: Simple Linear Regression Model\n",
        "model = LinearRegression().fit(X_train_encoded, y_train)\n",
        "\n",
        "# Linear MSE:\n",
        "lin_mse_test = mean_squared_error(y_test, model.predict(X_test_encoded))\n",
        "print('Linear Reg. Model MSE:', lin_mse_test)\n",
        "\n",
        "# Linear RMSE:\n",
        "lin_rmse = root_mean_squared_error(y_test, model.predict(X_test_encoded))\n",
        "print('Linear Reg. Model RMSE:', lin_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5R9uy9BYjZ4"
      },
      "outputs": [],
      "source": [
        "# Baseline 2: Global (Price) Mean Baseline\n",
        "baseline_global = DummyRegressor(strategy = 'mean').fit(X_train_encoded,y_train)\n",
        "y_test_pred = baseline_global.predict(X_test_encoded)\n",
        "\n",
        "# Global MSE\n",
        "global_mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "print('Global Price Baseline MSE:', global_mse_test)\n",
        "\n",
        "# Global RMSE\n",
        "global_rmse = root_mean_squared_error(y_test, y_test_pred)\n",
        "print('Category Price Baseline RMSE:', global_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQWEdHttipDH"
      },
      "outputs": [],
      "source": [
        "# Baseline 3: Category (Price) Mean Baseline\n",
        "baseline_cat = y_train.groupby(X_train['Category']).mean()\n",
        "\n",
        "y_test_pred_cat = X_test['Category'].map(baseline_cat).fillna(y_train.mean())\n",
        "\n",
        "# Cat MSE\n",
        "cat_mse_test = mean_squared_error(y_test, y_test_pred_cat)\n",
        "print('Category Price Baseline MSE:', cat_mse_test)\n",
        "\n",
        "# Cat RMSE\n",
        "cat_rmse = root_mean_squared_error(y_test, y_test_pred_cat)\n",
        "print('Category Price Baseline RMSE:', cat_rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FysxVx1snDwH"
      },
      "source": [
        "**XGB Boosted & Random Forest Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScIr5sasr5Pv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title #####GridSearch Optimize\n",
        "#GridSearch Optimizer (Commented out takes ~30 mins to run)\n",
        "'''\n",
        "# Optimizing with Grid Search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'n_estimators': [100, 200]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=XGBRegressor(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_encoded, y_train)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=XGBRegressor(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_encoded, y_train)\n",
        "print(grid_search.best_params_)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iea16VHm8cBX"
      },
      "source": [
        "XGB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdjR-5fo3-vr"
      },
      "outputs": [],
      "source": [
        "# XGB fit\n",
        "xgb = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb.fit(X_train_encoded, y_train)\n",
        "\n",
        "xgb_preds = xgb.predict(X_test_encoded)\n",
        "\n",
        "# Performance Evaluation (MSE, etc.)\n",
        "xgb_mse = mean_squared_error(y_test, xgb_preds)\n",
        "xgb_rmse = root_mean_squared_error(y_test, xgb_preds)\n",
        "\n",
        "print(\"XGBoost MSE:\", xgb_mse)\n",
        "print(\"XGBoost RMSE:\", xgb_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obe9dEdhgOXr"
      },
      "source": [
        "Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5R9GnXWgSSY"
      },
      "outputs": [],
      "source": [
        "# Random Forest Model\n",
        "forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest.fit(X_train_encoded, y_train)\n",
        "\n",
        "forest_preds = forest.predict(X_test_encoded)\n",
        "\n",
        "# Performance Evaluation (MSE, etc.)\n",
        "forest_mse = mean_squared_error(y_test, forest_preds)\n",
        "forest_rmse = root_mean_squared_error(y_test, forest_preds)\n",
        "\n",
        "print(\"Random Forest MSE:\", forest_mse)\n",
        "print(\"Random Forest RMSE:\", forest_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8yTBfD3YrLT"
      },
      "source": [
        "XGB Model RMSE < Random Forest RMSE  so we decided to move forward with the XGB model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison Overview:"
      ],
      "metadata": {
        "id": "zVfaEf4HUrkH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRrExklbcjdj"
      },
      "outputs": [],
      "source": [
        "# Model RMSE comparison chart\n",
        "rmse_values = {\n",
        "    'Linear': lin_rmse,\n",
        "    'Global Base': global_rmse,\n",
        "    'Category Base': cat_rmse,\n",
        "    'Random Forest': forest_rmse,\n",
        "    'XGBoost': xgb_rmse\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(rmse_values.keys(), rmse_values.values())\n",
        "plt.ylabel(\"RMSE ($)\")\n",
        "plt.title(\"Model RMSE Comparison\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C3QukH8jGwY"
      },
      "source": [
        "### XGB Model Results and Intepretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xumSMylHlxra"
      },
      "source": [
        "**Predicted vs Actual Mean Prices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geLv1RMwpXyH"
      },
      "outputs": [],
      "source": [
        "# Prepare dataframe to compare predicted prices for XGB and Random Forest\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Actual Price': y_test.values,\n",
        "    'XGBoost Prediction': xgb_preds,\n",
        "    'Category': X_test['Category'].values\n",
        "})\n",
        "\n",
        "# Combine data into long dataframe for plotting\n",
        "final_df = comparison_df.melt(\n",
        "    id_vars=['Actual Price', 'Category'],\n",
        "    value_vars=['XGBoost Prediction'],\n",
        "    var_name='Model',\n",
        "    value_name='Predicted Price'\n",
        ")\n",
        "\n",
        "# Create plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.scatterplot(\n",
        "    data=final_df,\n",
        "    x='Actual Price',\n",
        "    y='Predicted Price',\n",
        "    hue='Category',\n",
        "    style='Model',\n",
        "    alpha=0.8,\n",
        "    palette='tab10',\n",
        "    s=80\n",
        ")\n",
        "\n",
        "# Create perfect prediction line\n",
        "min_val = min(final_df['Actual Price'].min(), final_df['Predicted Price'].min())\n",
        "max_val = max(final_df['Actual Price'].max(), final_df['Predicted Price'].max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val],\n",
        "         color='gray', linestyle='--', label='Perfect Prediction')\n",
        "\n",
        "plt.title('Predicted vs Actual Prices by Category')\n",
        "plt.xlabel('Actual Mean Price')\n",
        "plt.ylabel('Predicted Mean Price')\n",
        "plt.grid(True)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPncWh4Bl9Ah"
      },
      "outputs": [],
      "source": [
        "# Category-level summary statistics\n",
        "category_summary = comparison_df.groupby('Category').agg(\n",
        "    predicted_mean_price = ('Actual Price', 'mean'),\n",
        "    median_price = ('Actual Price', 'median'),\n",
        "    min_price = ('Actual Price', 'min'),\n",
        "    max_price = ('Actual Price', 'max'),\n",
        "    iqr = ('Actual Price', lambda x: x.quantile(0.75) - x.quantile(0.25)),\n",
        "    count = ('Actual Price', 'count')\n",
        ").sort_values('predicted_mean_price')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6akCYAx2z8f"
      },
      "outputs": [],
      "source": [
        "# Predicted vs. Actual Mean Prices Chart\n",
        "# Actual mean prices\n",
        "actual_means = (\n",
        "    lagar_df.groupby(\"Category\")[\"Price\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"Price\": \"actual_mean_price\"})\n",
        ")\n",
        "\n",
        "# Merge table\n",
        "category_summary_with_actuals = category_summary.merge(\n",
        "    actual_means, on=\"Category\", how=\"left\"\n",
        ")\n",
        "\n",
        "# Isolate predicted vs actual\n",
        "predicted_vs_actual = category_summary_with_actuals.set_index('Category')[['predicted_mean_price', 'actual_mean_price']]\n",
        "predicted_vs_actual['difference'] = (predicted_vs_actual['predicted_mean_price'] - predicted_vs_actual['actual_mean_price'])\n",
        "predicted_vs_actual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwBrweBAkvBH"
      },
      "source": [
        "**Residual-Informed Pricing Ranges**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Miq0fPcDkvXa"
      },
      "outputs": [],
      "source": [
        "# Residuals for XGBoost model\n",
        "comparison_df['Residual'] = comparison_df['Actual Price'] - comparison_df['XGBoost Prediction']\n",
        "\n",
        "# Absolute residuals (for buffer calculations)\n",
        "comparison_df['Abs Residual'] = comparison_df['Residual'].abs()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBjKhPhvl9zQ"
      },
      "outputs": [],
      "source": [
        "# Category specific residuals & expected pricing ranges\n",
        "residual_buffer = comparison_df.groupby('Category')['Abs Residual'].mean().rename('Avg Residual')\n",
        "\n",
        "# Expected Pricing Ranges Per Category\n",
        "final_price_ranges = category_summary.join(residual_buffer)\n",
        "final_price_ranges['Expected Low']  = final_price_ranges['predicted_mean_price'] - final_price_ranges['Avg Residual']\n",
        "final_price_ranges['Expected High'] = final_price_ranges['predicted_mean_price'] + final_price_ranges['Avg Residual']\n",
        "\n",
        "final_price_ranges[['Expected Low', 'predicted_mean_price', 'Expected High','Avg Residual']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort categories by predicted mean price\n",
        "plot_df = final_price_ranges.sort_values('predicted_mean_price')\n",
        "\n",
        "# X positions\n",
        "x = range(len(plot_df))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Error bars (Expected Low to Expected High)\n",
        "plt.errorbar(\n",
        "    x=x,\n",
        "    y=plot_df['predicted_mean_price'],\n",
        "    yerr=plot_df['Avg Residual'],\n",
        "    fmt='o',\n",
        "    capsize=5\n",
        ")\n",
        "\n",
        "# Labels & formatting\n",
        "plt.xticks(x, plot_df.index, rotation=45, ha='right')\n",
        "plt.ylabel('Price')\n",
        "plt.xlabel('Category')\n",
        "plt.title('Expected Pricing Range by Category (Mean Â± Avg Absolute Residual)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWZfO37oepyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSjFieGUmGIR"
      },
      "outputs": [],
      "source": [
        "# Plot category residuals\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=comparison_df, y='Category', x='Residual')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Residual Distribution by Category (XGBoost)\")\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiNYO13LYZgE"
      },
      "source": [
        "### Feature Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqp-sY6ilt0X"
      },
      "source": [
        "**Permutation Feature Importancce**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute permutation importance\n",
        "result = permutation_importance(\n",
        "    xgb,\n",
        "    X_test_encoded,\n",
        "    y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "feature_names = []\n",
        "\n",
        "cat_names = encoder.named_transformers_['onehotencoder'].get_feature_names_out(['Size Guide','Category'])\n",
        "feature_names.extend(cat_names)\n",
        "feature_names.extend(['Number of pictures'])\n",
        "\n",
        "desc_names = encoder.named_transformers_['pipeline-1'].named_steps['Tfidf'].get_feature_names_out()\n",
        "feature_names.extend([f'Description_{n}' for n in desc_names])\n",
        "\n",
        "# TF-IDF features for Materials\n",
        "mat_names = encoder.named_transformers_['pipeline-2'].named_steps['Tfidf'].get_feature_names_out()\n",
        "feature_names.extend([f'Materials_{n}' for n in mat_names])\n",
        "\n",
        "# Image embedding placeholders\n",
        "img_cols = [f'img_{i}' for i in range(img_train.shape[1])]\n",
        "feature_names.extend(img_cols)\n",
        "\n",
        "# Permutation importance\n",
        "result = permutation_importance(\n",
        "    xgb,\n",
        "    X_test_encoded,\n",
        "    y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance_mean\": result.importances_mean,\n",
        "    \"importance_std\": result.importances_std\n",
        "}).sort_values(by=\"importance_mean\", ascending=False)\n",
        "\n",
        "# Group features into respective categories\n",
        "groups = []\n",
        "for f in perm_importance_df['feature']:\n",
        "    if 'Category' in f:\n",
        "        groups.append('Category')\n",
        "    elif 'Size Guide' in f:\n",
        "        groups.append('Size Guide')\n",
        "    elif 'Description' in f:\n",
        "        groups.append('Description')\n",
        "    elif 'Materials' in f:\n",
        "        groups.append('Materials')\n",
        "    elif 'img' in f:\n",
        "        groups.append('Image')\n",
        "    elif f == 'Number of pictures':\n",
        "        groups.append('Number of pictures')\n",
        "    else:\n",
        "        groups.append('Other')\n",
        "\n",
        "perm_importance_df['Group'] = groups\n",
        "\n",
        "# Aggregate by group\n",
        "grouped_importance = perm_importance_df.groupby('Group')['importance_mean'].sum()\n",
        "\n",
        "# Horizontal bar plot\n",
        "grouped_importance.sort_values().plot(kind='barh')\n",
        "plt.xlabel(\"Sum of Mean Permutation Importance\")\n",
        "plt.title(\"Feature Importance by Group\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MRsSDgW_0NmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgDni69Y_pk"
      },
      "source": [
        "**Interpreting\n",
        " Coefficients of Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wGsax5Wc33E"
      },
      "outputs": [],
      "source": [
        "# import OneHotEncoder\n",
        "ohe = OneHotEncoder(drop = 'first', sparse_output = False)\n",
        "\n",
        "# make column selector\n",
        "selector = make_column_transformer(\n",
        "    (ohe, ['Category', 'Description', 'Number of pictures', 'Materials', 'Size Guide']),\n",
        "     remainder = 'passthrough')\n",
        "\n",
        "# fit and transform\n",
        "XT = selector.fit_transform(X_train)\n",
        "\n",
        "# fit model\n",
        "model_t = LinearRegression().fit(XT, y_train)\n",
        "\n",
        "feature_names = selector.get_feature_names_out()\n",
        "\n",
        "#Scale coefs\n",
        "XT_std = XT.std(axis=0)\n",
        "standardized_coef = model_t.coef_ * XT_std\n",
        "\n",
        "# create df\n",
        "std_coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Scaled Coefficient': standardized_coef })\n",
        "\n",
        "std_coef_df['Group'] = std_coef_df['Feature'].apply(lambda x: x.split('__')[1].split('_')[0])\n",
        "\n",
        "grouped_coefs = std_coef_df.groupby('Group')['Scaled Coefficient'].apply(lambda x: x.abs().sum()).sort_values(ascending=True)\n",
        "\n",
        "# Bar plot by group\n",
        "plt.barh(grouped_coefs.index, grouped_coefs.values)\n",
        "plt.xlabel(\"Sum of Absolute Scaled Coefficients\")\n",
        "plt.title(\"Feature Importance by Group\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXrAIE1hbdws"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Select numeric columns only\n",
        "numeric_df = lagar_df.select_dtypes(include=['int64','float64'])\n",
        "\n",
        "corr = numeric_df.corr()\n",
        "\n",
        "sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}